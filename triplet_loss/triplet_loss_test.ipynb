{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend, layers, metrics\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import Xception, VGG16\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Lambda\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(input_shape):\n",
    "    \"\"\" Returns the image encoding model \"\"\"\n",
    "\n",
    "    pretrained_model = Xception(\n",
    "        input_shape=input_shape,\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        pooling='avg',\n",
    "    )\n",
    "    print(len(pretrained_model.layers))\n",
    "\n",
    "    for i in range(len(pretrained_model.layers)-27):\n",
    "        pretrained_model.layers[i].trainable = False\n",
    "\n",
    "    encode_model = Sequential([\n",
    "        pretrained_model,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        layers.Dropout(0.5),\n",
    "\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, activation=\"relu\", kernel_regularizer=l2(0.001)),\n",
    "        layers.Dropout(0.5),\n",
    "\n",
    "        layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n",
    "    ], name=\"Encode_Model\")\n",
    "\n",
    "    print(len(encode_model.layers))\n",
    "    return encode_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "8\n",
      "Model: \"Siamese_Network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " Anchor_Input (InputLayer)   [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " Positive_Input (InputLayer  [(None, 128, 128, 3)]        0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Negative_Input (InputLayer  [(None, 128, 128, 3)]        0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Encode_Model (Sequential)   (None, 256)                  2204394   ['Anchor_Input[0][0]',        \n",
      "                                                          4          'Positive_Input[0][0]',      \n",
      "                                                                     'Negative_Input[0][0]']      \n",
      "                                                                                                  \n",
      " distance_layer (DistanceLa  ((None,),                    0         ['Encode_Model[3][0]',        \n",
      " yer)                         (None,))                               'Encode_Model[4][0]',        \n",
      "                                                                     'Encode_Model[5][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22043944 (84.09 MB)\n",
      "Trainable params: 9583800 (36.56 MB)\n",
      "Non-trainable params: 12460144 (47.53 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "\n",
    "def get_siamese_network(input_shape = (128, 128, 3)):\n",
    "    encoder = get_encoder(input_shape)\n",
    "\n",
    "    # Input Layers for the images\n",
    "    anchor_input   = layers.Input(input_shape, name=\"Anchor_Input\")\n",
    "    positive_input = layers.Input(input_shape, name=\"Positive_Input\")\n",
    "    negative_input = layers.Input(input_shape, name=\"Negative_Input\")\n",
    "\n",
    "    ## Generate the encodings (feature vectors) for the images\n",
    "    encoded_a = encoder(anchor_input)\n",
    "    encoded_p = encoder(positive_input)\n",
    "    encoded_n = encoder(negative_input)\n",
    "\n",
    "    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n",
    "    distances = DistanceLayer()(\n",
    "        encoder(anchor_input),\n",
    "        encoder(positive_input),\n",
    "        encoder(negative_input)\n",
    "    )\n",
    "\n",
    "    # Creating the Model\n",
    "    siamese_network = Model(\n",
    "        inputs  = [anchor_input, positive_input, negative_input],\n",
    "        outputs = distances,\n",
    "        name = \"Siamese_Network\"\n",
    "    )\n",
    "    return siamese_network\n",
    "\n",
    "siamese_network = get_siamese_network()\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    # Builds a Siamese model based on a base-model\n",
    "    def __init__(self, siamese_network, margin=1.0):\n",
    "        super(SiameseModel, self).__init__()\n",
    "\n",
    "        self.margin = margin\n",
    "        self.siamese_network = siamese_network\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape get the gradients when we compute loss, and uses them to update the weights\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # Get the two distances from the network, then compute the triplet loss\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics so the reset_states() can be called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "siamese_model = SiameseModel(siamese_network)\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-3, epsilon=1e-01)\n",
    "siamese_model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encoder(model):\n",
    "    encoder = get_encoder((128, 128, 3))\n",
    "    i=0\n",
    "    for e_layer in model.layers[0].layers[3].layers:\n",
    "        layer_weight = e_layer.get_weights()\n",
    "        encoder.layers[i].set_weights(layer_weight)\n",
    "        i+=1\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "encoder = extract_encoder(siamese_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seeds to enable consistency while testing.\n",
    "random.seed(5)\n",
    "np.random.seed(5)\n",
    "tf.random.set_seed(5)\n",
    "ROOT = 'dataset'\n",
    "def read_image(path):\n",
    "    image_1 = cv2.imread(path)\n",
    "    image = cv2.resize(image_1, (128, 128))\n",
    "\n",
    "    if image is not None:\n",
    "        return image\n",
    "    else:\n",
    "        print(f\"Error: Unable to load image at path {path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2e2f58590>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.load_weights(\"Recognition_siamese_model_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2e31caa50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_weights(\"encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to read and preprocess an image\n",
    "def read_and_preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "# Function to encode an image using the trained encoder\n",
    "def encode_image(encoder, image_path):\n",
    "    image = read_and_preprocess_image(image_path)\n",
    "    encoded_image = encoder.predict(np.expand_dims(image, axis=0))[0]\n",
    "    return encoded_image\n",
    "\n",
    "# Function to get the list of training images\n",
    "def get_training_images(root_folder):\n",
    "    training_images = []\n",
    "    classes = os.listdir(os.path.join(root_folder, 'train'))\n",
    "    for class_folder in classes:\n",
    "        class_path = os.path.join(root_folder, 'train', class_folder)\n",
    "        # for image_file in os.listdir(class_path):\n",
    "        image1_path = os.path.join(class_path, random.choice(os.listdir(os.path.join(root_folder, 'train', class_folder))))\n",
    "        training_images.append(image1_path)\n",
    "        # image2_path = os.path.join(class_path, random.choice(os.listdir(os.path.join(root_folder, 'train', class_folder))))\n",
    "        # training_images.append(image2_path)\n",
    "\n",
    "    return training_images, classes\n",
    "\n",
    "\n",
    "# Root folder containing train, test, and validation folders\n",
    "root_folder = \"dataset/\"\n",
    "\n",
    "# Get the list of training images\n",
    "training_images, classes = get_training_images(root_folder)\n",
    "\n",
    "# Encode each training image\n",
    "encoded_training_images = []\n",
    "\n",
    "for image_path in training_images:\n",
    "    encoded_image = encode_image(encoder, image_path)\n",
    "    encoded_training_images.append(encoded_image)\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "encoded_training_images = np.array(encoded_training_images)\n",
    "\n",
    "# Now, encoded_training_images contains the encoded representations of all training images\n",
    "# You can use this array for further analysis or classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Compute Euclidean distance between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - vector1, vector2: Numpy arrays representing feature vectors.\n",
    "    \n",
    "    Returns:\n",
    "    - Euclidean distance between vector1 and vector2.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(vector1 - vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 152ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_dir = 'dataset/test'\n",
    "test_class = random.choice(os.listdir(test_dir))\n",
    "test_image = random.choice(os.listdir(os.path.join(test_dir,test_class)))\n",
    "\n",
    "test_image = preprocess_input(read_image(os.path.join(test_dir, test_class, test_image)))\n",
    "encoded_test_image = encoder.predict(np.expand_dims(test_image, axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Class Distance = 0.7044041156768799\n",
      "Minimum Distance: 0.28641846776008606\n"
     ]
    }
   ],
   "source": [
    "from math import inf\n",
    "\n",
    "\n",
    "distances_to_classes = []\n",
    "\n",
    "maximum_dist = inf\n",
    "\n",
    "for i, encoded_class_image in enumerate(encoded_training_images):\n",
    "    distance = compute_distance(encoded_test_image, encoded_class_image)\n",
    "    distances_to_classes.append(distance)\n",
    "\n",
    "    if classes[i] == test_class:\n",
    "        print(f\"Test Class Distance = {distance}\")\n",
    "\n",
    "\n",
    "    if maximum_dist > distance:\n",
    "        maximum_dist = distance\n",
    "\n",
    "print(f\"Minimum Distance: {maximum_dist}\")\n",
    "\n",
    "print(len(distances_to_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_class = classes[np.argmin(distances_to_classes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pins_Alvaro Morte\n",
      "pins_Chris Hemsworth\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
